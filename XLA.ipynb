{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Tạo class Convolution để định nghĩa lớp CONV2D\n",
        "Class Convolution sẽ nhận các tham số:\n",
        "- input shape: Là một mảng 2 phần tử height và width mô tả kích thước của ảnh input\n",
        "- filter_size: Kích thước kernel cần dùng\n",
        "- num_filter: Số lớp chồng feature map"
      ],
      "metadata": {
        "id": "IDk8K7GT8WP_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Công Thức Chuỗi (Chain Rule)\n",
        " Gradient tổng Loss (L) theo một trọng số filter cụ thể (W_{m,n}) là tổng ảnh hưởng qua mọi điểm đầu ra O_{i,j}:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial W_{m,n}} = \\sum_{i,j} \\frac{\\partial L}{\\partial O_{i,j}} \\cdot \\frac{\\partial O_{i,j}}{\\partial W_{m,n}}\n",
        "$$\n",
        "\n",
        "## 2. Gradient Cục Bộ (Local Gradient)\n",
        "Từ công thức lan truyền xuôi (Forward Pass):\n",
        "$$\n",
        "O_{i,j} = \\sum_{m',n'} X_{i+m', j+n'} \\cdot W_{m',n'}\n",
        "$$\n",
        "Ta có đạo hàm riêng của O_{i,j} theo W_{m,n} chính là giá trị Input tương ứng:\n",
        "$$\n",
        "\\frac{\\partial O_{i,j}}{\\partial W_{m,n}} = X_{i+m, j+n}\n",
        "$$\n",
        "\n",
        "## 3. Nhận Dạng Phép Toán (Tương quan chéo)\n",
        "Thay kết quả vào Chain Rule, ta được công thức tính Gradient của Filter:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial W_{m,n}} = \\sum_{i,j} X_{i+m, j+n} \\cdot \\frac{\\partial L}{\\partial O_{i,j}}\n",
        "$$\n",
        "Công thức này chính là phép **Tương quan chéo (Cross-Correlation)** giữa Input (X) và Gradient đầu ra $(\\frac{\\partial L}{\\partial O})$."
      ],
      "metadata": {
        "id": "sL5sMhErAI5D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "on8z2bg-8qa7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.signal import correlate2d\n",
        "\n",
        "class Convolution:\n",
        "\n",
        "    def __init__(self, input_shape, filter_size, num_filters):\n",
        "        input_height, input_width = input_shape\n",
        "        self.num_filters = num_filters\n",
        "        self.input_shape = input_shape\n",
        "\n",
        "        self.filter_shape = (num_filters, filter_size, filter_size)\n",
        "        self.output_shape = (num_filters, input_height, input_width)\n",
        "\n",
        "        self.filters = np.random.randn(*self.filter_shape)\n",
        "        self.biases = np.random.randn(*self.output_shape)\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        self.input_data = input_data\n",
        "        output = np.zeros(self.output_shape)\n",
        "        for i in range(self.num_filters):\n",
        "            # Sử dụng mode = same để đảm bảo không bị mất pixel sau khi thực hiện tích chập (zero - pad)\n",
        "            output[i] = correlate2d(self.input_data, self.filters[i], mode=\"same\")\n",
        "\n",
        "        # Áp dụng Relu vào đầu ra\n",
        "        output = np.maximum(output, 0)\n",
        "        return output\n",
        "\n",
        "    def backward(self, dL_out, lr):\n",
        "        dL_input = np.zeros_like(self.input_data)\n",
        "        dL_filters = np.zeros_like(self.filters)\n",
        "\n",
        "        for i in range(self.num_filters):\n",
        "                # Tính gradient cho filter\n",
        "                dL_filters[i] = correlate2d(self.input_data, dL_out[i],mode=\"valid\")\n",
        "\n",
        "                # Tính gradient cho input để backprop\n",
        "                dL_input += correlate2d(dL_out[i],self.filters[i], mode=\"same\")\n",
        "\n",
        "        # Update tham số\n",
        "        self.filters -= lr * dL_filters\n",
        "        self.biases -= lr * dL_out\n",
        "\n",
        "        # trả gradient input nếu cần lan truyền ngược\n",
        "        return dL_input\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Lớp Max Pooling\n",
        "\n",
        "Max Pooling không có tham số để học, chỉ tính gradient của Loss theo Input (L/X) để truyền ngược.\n",
        "\n",
        "1. Công thức tổng quát:\n",
        "Gradient của Loss (L) theo một phần tử Input (X_{c, i, j}) được tính bằng:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial \\mathbf{X}_{c, i, j}} = \\begin{cases} \\frac{\\partial L}{\\partial \\mathbf{O}_{c, i', j'}} & \\text{nếu } \\mathbf{X}_{c, i, j} \\text{ là } \\max \\text{ trong cửa sổ } (i', j') \\\\ 0 & \\text{trong trường hợp còn lại} \\end{cases}\n",
        "$$\n",
        "\n",
        "2. Diễn giải theo hoạt động (Sử dụng Masking):\n",
        "Gradient $(\\frac{\\partial L}{\\partial \\mathbf{O}})$ chỉ được nhân với mặt nạ (mask) của các vị trí winning.\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial \\mathbf{X}_{\\text{patch}}} = \\frac{\\partial L}{\\partial \\mathbf{O}_{\\text{cell}}} \\times \\mathbf{Mask}\n",
        "$$\n",
        "Trong đó:\n",
        "$$\\mathbf{Mask}_{\\text{patch}} = \\begin{cases} 1 & \\text{nếu } \\mathbf{X}_{\\text{patch}} \\text{ là } \\max(\\mathbf{X}_{\\text{patch}}) \\\\ 0 & \\text{ngược lại} \\end{cases}$$"
      ],
      "metadata": {
        "id": "o7EIIRZeEmIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaxPool:\n",
        "    def __init__(self, pool_size):\n",
        "        self.pool_size = pool_size\n",
        "    def forward(self, input_data):\n",
        "        self.input_data = input_data\n",
        "        self.num_channels, self.input_height, self.input_width = input_data.shape\n",
        "        # Chỗ này chia cho pool size vì sau khi max pool đầu ra sẽ bị downsample đi ngần nấy lần\n",
        "        self.output_height = self.input_height // self.pool_size\n",
        "        self.output_width = self.input_width // self.pool_size\n",
        "\n",
        "        self.output = np.zeros((self.num_channels, self.output_height, self.output_width))\n",
        "\n",
        "        # lặp qua các channel\n",
        "        for c in range(self.num_channels):\n",
        "            # lặp theo height\n",
        "            for i in range(self.output_height):\n",
        "                # lặp theo width\n",
        "                for j in range(self.output_width):\n",
        "                    start_i = i * self.pool_size\n",
        "                    start_j = j * self.pool_size\n",
        "                    end_i = start_i + self.pool_size\n",
        "                    end_j = start_j + self.pool_size\n",
        "\n",
        "                    # Sau khi xác lập được các toạ độ của patch cần thực hiện maxpool, tạo patch\n",
        "                    patch = input_data[c, start_i:end_i, start_j:end_j]\n",
        "\n",
        "                    # Chọn winning pixel\n",
        "                    self.output[c, i, j] = np.max(patch)\n",
        "\n",
        "        return self.output\n",
        "    def backward(self, dL_dout, lr):\n",
        "        dL_dinput = np.zeros_like(self.input_data)\n",
        "\n",
        "        for c in range(self.num_channels):\n",
        "            for i in range(self.output_height):\n",
        "                for j in range(self.output_width):\n",
        "                    start_i = i * self.pool_size\n",
        "                    start_j = j * self.pool_size\n",
        "\n",
        "                    end_i = start_i + self.pool_size\n",
        "                    end_j = start_j + self.pool_size\n",
        "                    patch = self.input_data[c, start_i:end_i, start_j:end_j]\n",
        "\n",
        "                    mask = patch == np.max(patch)\n",
        "\n",
        "                    dL_dinput[c,start_i:end_i, start_j:end_j] = dL_dout[c, i, j] * mask\n",
        "\n",
        "        return dL_dinput"
      ],
      "metadata": {
        "id": "EFmF5uKt8tBW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lớp Fully Connected (Dense) Backpropagation\n",
        "\n",
        " 1. Tính Gradient Loss theo Pre-Activation (Sử dụng Softmax)\n",
        " Đây là tín hiệu lỗi sau khi đi qua đạo hàm của hàm kích hoạt (Softmax).\n",
        " Công thức sử dụng đạo hàm Softmax trên vector (Jaccobian matrix):\n",
        "$$\n",
        "\\mathbf{\\delta} = \\frac{\\partial L}{\\partial \\mathbf{Z}} = \\frac{\\partial L}{\\partial \\mathbf{Y}} \\cdot \\frac{\\partial \\mathbf{Y}}{\\partial \\mathbf{Z}}\n",
        "$$\n",
        " Với $\\frac{\\partial \\mathbf{Y}}{\\partial \\mathbf{Z}}$ là ma trận Jacobian của Softmax.\n",
        "\n",
        " 2. Gradient theo Trọng số (W)\n",
        " Cập nhật Trọng số W:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial \\mathbf{W}} = \\mathbf{\\delta} \\cdot \\mathbf{X}^{T}\n",
        "$$\n",
        "\n",
        "3. Gradient theo Bias (B)\n",
        " Cập nhật Bias B:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial \\mathbf{B}} = \\mathbf{\\delta}\n",
        "$$\n",
        "\n",
        "4. Gradient theo Input (X)\n",
        "Gradient truyền về lớp trước (X là input đã được làm phẳng):\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial \\mathbf{X}} = \\mathbf{W}^{T} \\cdot \\mathbf{\\delta}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "wO9VC6YoFXiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Fully_Connected:\n",
        "\n",
        "    def __init__(self, input_size, output_size=10):\n",
        "        self.input_size = input_size # Size của input, số này phải được tính bằng map * chiều cao * chiều rộng ảnh\n",
        "        self.output_size = output_size # Size của output, chọn 10 vì có 10 chữ số 0 - 9\n",
        "        self.weights = np.random.randn(output_size, self.input_size)\n",
        "        self.biases = np.random.rand(output_size, 1)\n",
        "    def softmax(self, z):\n",
        "        # Code chống numerical instability\n",
        "        shifted_z = z - np.max(z)\n",
        "        exp_values = np.exp(shifted_z)\n",
        "        sum_exp_values = np.sum(exp_values, axis=0)\n",
        "        log_sum_exp = np.log(sum_exp_values)\n",
        "\n",
        "        # Tính softmax\n",
        "        probabilities = exp_values / sum_exp_values\n",
        "\n",
        "        return probabilities\n",
        "\n",
        "    def softmax_derivative(self, s):\n",
        "        return np.diagflat(s) - np.dot(s, s.T)\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        self.input_data = input_data\n",
        "        flattened_input = input_data.flatten().reshape(1, -1)\n",
        "        self.z = np.dot(self.weights, flattened_input.T) + self.biases\n",
        "        self.output = self.softmax(self.z)\n",
        "        return self.output\n",
        "    def backward(self, dL_out, lr):\n",
        "        dL_y = np.dot(self.softmax_derivative(self.output), dL_out)\n",
        "        dL_w = np.dot(dL_y, self.input_data.flatten().reshape(1, -1))\n",
        "        dL_db = dL_y\n",
        "\n",
        "        dL_input = np.dot(self.weights.T, dL_y)\n",
        "        dL_input = dL_input.reshape(self.input_data.shape)\n",
        "\n",
        "        self.weights -= lr * dL_w\n",
        "        self.biases -= lr * dL_db\n",
        "\n",
        "        return dL_input"
      ],
      "metadata": {
        "id": "alxOk3IF8uHD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hàm Mất Mát Cross-Entropy (CCE Loss)\n",
        "\n",
        "Công thức tính Loss trung bình (Mean Cross-Entropy Loss) cho nhiều mẫu (num_samples):\n",
        "$$\n",
        "L = - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{c=1}^{C} y_{i,c} \\log(\\hat{y}_{i,c})\n",
        "$$\n",
        "\n",
        "Trong đó:\n",
        "* $N$: Tổng số mẫu (num_samples).\n",
        "* $C$: Tổng số lớp (classes).\n",
        "* $y_{i,c}$: Nhãn thực tế (targets) cho mẫu thứ $i$và lớp thứ$c$ (thường là 1 nếu đúng, 0 nếu sai).\n",
        "* $\\hat{y}_{i,c}$: Xác suất dự đoán (predictions) cho mẫu thứ i và lớp thứ c (sau hàm Softmax).\n",
        "* **Ghi chú code:** Hàm `np.clip` được dùng để tránh $\\log(0)$ (numerical instability).\n",
        "\n",
        "---\n",
        "\n",
        "## Gradient của Cross-Entropy Loss\n",
        " Gradient của Loss theo xác suất dự đoán ($\\hat{y}$), sau khi đã tính đạo hàm qua hàm Softmax (thường là bước đầu tiên trong backward pass):\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial \\hat{\\mathbf{Y}}} = - \\frac{1}{N} \\cdot \\frac{\\mathbf{Y}}{\\hat{\\mathbf{Y}}}\n",
        "$$\n",
        " Hoặc đối với từng phần tử:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial \\hat{y}_{i,c}} = - \\frac{1}{N} \\cdot \\frac{y_{i,c}}{\\hat{y}_{i,c}}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "q_-kqR1SFv_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss(predictions, targets):\n",
        "\n",
        "    num_samples = 10\n",
        "\n",
        "    # Avoid numerical instability by adding a small epsilon value\n",
        "    epsilon = 1e-7\n",
        "    predictions = np.clip(predictions, epsilon, 1 - epsilon)\n",
        "    loss = -np.sum(targets * np.log(predictions)) / num_samples\n",
        "    return loss\n",
        "\n",
        "def cross_entropy_loss_gradient(actual_labels, predicted_probs):\n",
        "    num_samples = actual_labels.shape[0]\n",
        "    gradient = -actual_labels / (predicted_probs + 1e-7) / num_samples\n",
        "\n",
        "    return gradient\n",
        "\n",
        "def train_network(X, y, conv, pool, full, lr=0.01, epochs=200):\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.0\n",
        "        correct_predictions = 0\n",
        "\n",
        "        for i in range(len(X)):\n",
        "            # Forward pass\n",
        "            conv_out = conv.forward(X[i])\n",
        "            pool_out = pool.forward(conv_out)\n",
        "            full_out = full.forward(pool_out)\n",
        "            loss = cross_entropy_loss(full_out.flatten(), y[i])\n",
        "            total_loss += loss\n",
        "\n",
        "            # Converting to One-Hot encoding\n",
        "            one_hot_pred = np.zeros_like(full_out)\n",
        "            one_hot_pred[np.argmax(full_out)] = 1\n",
        "            one_hot_pred = one_hot_pred.flatten()\n",
        "\n",
        "            num_pred = np.argmax(one_hot_pred)\n",
        "            num_y = np.argmax(y[i])\n",
        "\n",
        "            if num_pred == num_y:\n",
        "                correct_predictions += 1\n",
        "            # Backward pass\n",
        "            gradient = cross_entropy_loss_gradient(y[i], full_out.flatten()).reshape((-1, 1))\n",
        "            full_back = full.backward(gradient, lr)\n",
        "            pool_back = pool.backward(full_back, lr)\n",
        "            conv_back = conv.backward(pool_back, lr)\n",
        "\n",
        "        # Print epoch statistics\n",
        "        average_loss = total_loss / len(X)\n",
        "        accuracy = correct_predictions / len(X_train) * 100.0\n",
        "        print(f\"Epoch {epoch + 1}/{epochs} - Loss: {average_loss:.4f} - Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "def predict(input_sample, conv, pool, full):\n",
        "    # Forward pass through Convolution and pooling\n",
        "    conv_out = conv.forward(input_sample)\n",
        "    pool_out = pool.forward(conv_out)\n",
        "    # Flattening\n",
        "    flattened_output = pool_out.flatten()\n",
        "    # Forward pass through fully connected layer\n",
        "    predictions = full.forward(flattened_output)\n",
        "    return predictions\n",
        "\n"
      ],
      "metadata": {
        "id": "C71XISGx8vMz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tải dữ liệu MNIST\n",
        "\n",
        " 1. Tải và Chia Dữ liệu\n",
        "Tải tập dữ liệu MNIST: Lấy ma trận ảnh (X) và nhãn số nguyên (y).\n",
        " Phân chia:\n",
        " - Tập huấn luyện (X_train, y_train): 5000 mẫu đầu tiên.\n",
        " - Tập kiểm tra (X_test, y_test): 5000 mẫu tiếp theo.\n",
        "\n",
        " 2. Chuẩn hóa (Normalization): Đưa giá trị pixel từ [0, 255] về [0.0, 1.0].\n",
        " Giúp tối ưu hóa hội tụ nhanh hơn và ổn định hơn. Thực hiện: Chia X_train và X_test cho 255.0.\n",
        "\n",
        " 3. Mã hóa One-Hot (One-Hot Encoding): Chuyển nhãn số nguyên (0-9) thành vector nhị phân 10 chiều, đây là yêu cầu bắt buộc khi sử dụng hàm Softmax và Cross-Entropy Loss trong lớp đầu ra.\n",
        " Thực hiện: Sử dụng to_categorical() cho y_train và y_test.\n"
      ],
      "metadata": {
        "id": "EwrIYJrxKRRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras as keras\n",
        "import numpy as np\n",
        "\n",
        "# Load the Fashion MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()\n",
        "X_train = train_images[:5000] / 255.0\n",
        "y_train = train_labels[:5000]\n",
        "\n",
        "X_test = train_images[5000:10000] / 255.0\n",
        "y_test = train_labels[5000:10000]\n",
        "\n",
        "X_train.shape\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "y_test[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_W-o1Y98xRz",
        "outputId": "c0701a6e-90a7-4bff-d398-10041dd16bd2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Huấn luyện mạng CNN với 5 feature maps"
      ],
      "metadata": {
        "id": "-nwi3OypK3Rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv = Convolution([28,28], 2, 5)\n",
        "pool = MaxPool(2)\n",
        "full = Fully_Connected(5*14*14,10)\n",
        "train_network(X_train, y_train, conv, pool, full, epochs=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMqp5bd48yLs",
        "outputId": "be9f69f9-cc99-40cf-a7e3-7e1cb8aed914"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - Loss: 0.7690 - Accuracy: 17.70%\n",
            "Epoch 2/10 - Loss: 0.5848 - Accuracy: 33.30%\n",
            "Epoch 3/10 - Loss: 0.4588 - Accuracy: 46.86%\n",
            "Epoch 4/10 - Loss: 0.3860 - Accuracy: 55.24%\n",
            "Epoch 5/10 - Loss: 0.3436 - Accuracy: 60.90%\n",
            "Epoch 6/10 - Loss: 0.3178 - Accuracy: 64.62%\n",
            "Epoch 7/10 - Loss: 0.3005 - Accuracy: 67.60%\n",
            "Epoch 8/10 - Loss: 0.2881 - Accuracy: 69.66%\n",
            "Epoch 9/10 - Loss: 0.2790 - Accuracy: 71.18%\n",
            "Epoch 10/10 - Loss: 0.2718 - Accuracy: 72.08%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "index = 7\n",
        "image = train_images[index]\n",
        "\n",
        "print(f\"Kích thước ảnh: {image.shape}\")\n",
        "print(f\"Nhãn (Label) của ảnh: {train_labels[index]}\")\n",
        "\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.imshow(image, cmap=\"gray\")\n",
        "plt.title(f\"Ảnh (Nhãn: {train_labels[index]})\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "3rjtB5E8B9sU",
        "outputId": "019297c6-33de-4e63-ac21-e195d108a62d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kích thước ảnh: (28, 28)\n",
            "Nhãn (Label) của ảnh: 3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code để thử mô hình\n",
        "np.argmax(predict(image, conv, pool, full))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Pweu0hUCBXD",
        "outputId": "b6432981-16b8-4c1a-e48b-307b9b674a1f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int64(3)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lưu mô hình vào file pkl để dùng sau"
      ],
      "metadata": {
        "id": "uMS_3rAFIpPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir()\n",
        "\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# 1. Tập hợp các thành phần của mô hình vào một dictionary (hoặc list)\n",
        "model_components = {\n",
        "    'conv_layer': conv,\n",
        "    'pool_layer': pool, # MaxPool không có tham số học, nhưng vẫn lưu instance để tái tạo cấu trúc\n",
        "    'fc_layer': full\n",
        "}\n",
        "\n",
        "# 2. Định nghĩa tên file\n",
        "model_filename = 'cnn_model.pkl'\n",
        "\n",
        "# 3. Lưu trữ bằng pickle\n",
        "try:\n",
        "    with open(model_filename, 'wb') as file:\n",
        "        pickle.dump(model_components, file)\n",
        "    print(f\"Mô hình đã được lưu thành công vào file: {model_filename}\")\n",
        "except Exception as e:\n",
        "    print(f\"Lỗi khi lưu mô hình: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ebdKlBEHk_G",
        "outputId": "52932bb1-fc4f-41ff-ce9b-15b09f3cfe81"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mô hình đã được lưu thành công vào file: sample_data/cnn_model.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code để load mô hình\n",
        "\n",
        "Lưu ý: Khi load lại mô hình cần load cả định nghĩa các class bên trên kia"
      ],
      "metadata": {
        "id": "xBkiiem9IxPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Đoạn đầu này là để tái định nghĩa các lớp và hàm\n",
        "\n",
        "import numpy as np\n",
        "from scipy.signal import correlate2d\n",
        "\n",
        "class Convolution:\n",
        "\n",
        "    def __init__(self, input_shape, filter_size, num_filters):\n",
        "        input_height, input_width = input_shape\n",
        "        self.num_filters = num_filters\n",
        "        self.input_shape = input_shape\n",
        "\n",
        "        self.filter_shape = (num_filters, filter_size, filter_size)\n",
        "        self.output_shape = (num_filters, input_height, input_width)\n",
        "\n",
        "        self.filters = np.random.randn(*self.filter_shape)\n",
        "        self.biases = np.random.randn(*self.output_shape)\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        self.input_data = input_data\n",
        "        output = np.zeros(self.output_shape)\n",
        "        for i in range(self.num_filters):\n",
        "            output[i] = correlate2d(self.input_data, self.filters[i], mode=\"same\")\n",
        "        output = np.maximum(output, 0)\n",
        "        return output\n",
        "\n",
        "    def backward(self, dL_out, lr):\n",
        "        dL_input = np.zeros_like(self.input_data)\n",
        "        dL_filters = np.zeros_like(self.filters)\n",
        "\n",
        "        for i in range(self.num_filters):\n",
        "                dL_filters[i] = correlate2d(self.input_data, dL_out[i],mode=\"valid\")\n",
        "                dL_input += correlate2d(dL_out[i],self.filters[i], mode=\"same\")\n",
        "        self.filters -= lr * dL_filters\n",
        "        self.biases -= lr * dL_out\n",
        "        return dL_input\n",
        "\n",
        "class MaxPool:\n",
        "    def __init__(self, pool_size):\n",
        "        self.pool_size = pool_size\n",
        "    def forward(self, input_data):\n",
        "        self.input_data = input_data\n",
        "        self.num_channels, self.input_height, self.input_width = input_data.shape\n",
        "        self.output_height = self.input_height // self.pool_size\n",
        "        self.output_width = self.input_width // self.pool_size\n",
        "\n",
        "        self.output = np.zeros((self.num_channels, self.output_height, self.output_width))\n",
        "\n",
        "        for c in range(self.num_channels):\n",
        "            for i in range(self.output_height):\n",
        "                for j in range(self.output_width):\n",
        "                    start_i = i * self.pool_size\n",
        "                    start_j = j * self.pool_size\n",
        "                    end_i = start_i + self.pool_size\n",
        "                    end_j = start_j + self.pool_size\n",
        "                    patch = input_data[c, start_i:end_i, start_j:end_j]\n",
        "                    self.output[c, i, j] = np.max(patch)\n",
        "\n",
        "        return self.output\n",
        "    def backward(self, dL_dout, lr):\n",
        "        dL_dinput = np.zeros_like(self.input_data)\n",
        "\n",
        "        for c in range(self.num_channels):\n",
        "            for i in range(self.output_height):\n",
        "                for j in range(self.output_width):\n",
        "                    start_i = i * self.pool_size\n",
        "                    start_j = j * self.pool_size\n",
        "\n",
        "                    end_i = start_i + self.pool_size\n",
        "                    end_j = start_j + self.pool_size\n",
        "                    patch = self.input_data[c, start_i:end_i, start_j:end_j]\n",
        "\n",
        "                    mask = patch == np.max(patch)\n",
        "\n",
        "                    dL_dinput[c,start_i:end_i, start_j:end_j] = dL_dout[c, i, j] * mask\n",
        "\n",
        "        return dL_dinput\n",
        "class Fully_Connected:\n",
        "\n",
        "    def __init__(self, input_size, output_size=10):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.weights = np.random.randn(output_size, self.input_size)\n",
        "        self.biases = np.random.rand(output_size, 1)\n",
        "    def softmax(self, z):\n",
        "        shifted_z = z - np.max(z)\n",
        "        exp_values = np.exp(shifted_z)\n",
        "        sum_exp_values = np.sum(exp_values, axis=0)\n",
        "        log_sum_exp = np.log(sum_exp_values)\n",
        "        probabilities = exp_values / sum_exp_values\n",
        "        return probabilities\n",
        "\n",
        "    def softmax_derivative(self, s):\n",
        "        return np.diagflat(s) - np.dot(s, s.T)\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        self.input_data = input_data\n",
        "        flattened_input = input_data.flatten().reshape(1, -1)\n",
        "        self.z = np.dot(self.weights, flattened_input.T) + self.biases\n",
        "        self.output = self.softmax(self.z)\n",
        "        return self.output\n",
        "    def backward(self, dL_out, lr):\n",
        "        dL_y = np.dot(self.softmax_derivative(self.output), dL_out)\n",
        "        dL_w = np.dot(dL_y, self.input_data.flatten().reshape(1, -1))\n",
        "        dL_db = dL_y\n",
        "        dL_input = np.dot(self.weights.T, dL_y)\n",
        "        dL_input = dL_input.reshape(self.input_data.shape)\n",
        "        self.weights -= lr * dL_w\n",
        "        self.biases -= lr * dL_db\n",
        "\n",
        "        return dL_input\n",
        "\n",
        "def cross_entropy_loss(predictions, targets):\n",
        "\n",
        "    num_samples = 10\n",
        "    epsilon = 1e-7\n",
        "    predictions = np.clip(predictions, epsilon, 1 - epsilon)\n",
        "    loss = -np.sum(targets * np.log(predictions)) / num_samples\n",
        "    return loss\n",
        "\n",
        "def cross_entropy_loss_gradient(actual_labels, predicted_probs):\n",
        "    num_samples = actual_labels.shape[0]\n",
        "    gradient = -actual_labels / (predicted_probs + 1e-7) / num_samples\n",
        "\n",
        "    return gradient\n",
        "\n",
        "def train_network(X, y, conv, pool, full, lr=0.01, epochs=200):\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.0\n",
        "        correct_predictions = 0\n",
        "\n",
        "        for i in range(len(X)):\n",
        "            conv_out = conv.forward(X[i])\n",
        "            pool_out = pool.forward(conv_out)\n",
        "            full_out = full.forward(pool_out)\n",
        "            loss = cross_entropy_loss(full_out.flatten(), y[i])\n",
        "            total_loss += loss\n",
        "\n",
        "            one_hot_pred = np.zeros_like(full_out)\n",
        "            one_hot_pred[np.argmax(full_out)] = 1\n",
        "            one_hot_pred = one_hot_pred.flatten()\n",
        "\n",
        "            num_pred = np.argmax(one_hot_pred)\n",
        "            num_y = np.argmax(y[i])\n",
        "\n",
        "            if num_pred == num_y:\n",
        "                correct_predictions += 1\n",
        "            gradient = cross_entropy_loss_gradient(y[i], full_out.flatten()).reshape((-1, 1))\n",
        "            full_back = full.backward(gradient, lr)\n",
        "            pool_back = pool.backward(full_back, lr)\n",
        "            conv_back = conv.backward(pool_back, lr)\n",
        "\n",
        "        average_loss = total_loss / len(X)\n",
        "        accuracy = correct_predictions / len(X_train) * 100.0\n",
        "        print(f\"Epoch {epoch + 1}/{epochs} - Loss: {average_loss:.4f} - Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "def predict(input_sample, conv, pool, full):\n",
        "    conv_out = conv.forward(input_sample)\n",
        "    pool_out = pool.forward(conv_out)\n",
        "    flattened_output = pool_out.flatten()\n",
        "    predictions = full.forward(flattened_output)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "\n",
        "# Từ đây là để load mô hình\n",
        "import pickle\n",
        "\n",
        "# 1. Tên file cần tải\n",
        "model_filename = 'cnn_model.pkl'\n",
        "\n",
        "# 2. Tải mô hình bằng pickle\n",
        "try:\n",
        "    with open(model_filename, 'rb') as file:\n",
        "        loaded_model = pickle.load(file)\n",
        "\n",
        "    # 3. Gán lại các thành phần đã tải vào các biến\n",
        "    loaded_conv = loaded_model['conv_layer']\n",
        "    loaded_pool = loaded_model['pool_layer']\n",
        "    loaded_full = loaded_model['fc_layer']\n",
        "\n",
        "    print(\"Mô hình đã được tải lại thành công.\")\n",
        "    print(f\"Kích thước bộ lọc lớp CONV: {loaded_conv.filters.shape}\")\n",
        "    print(f\"Kích thước trọng số lớp FC: {loaded_full.weights.shape}\")\n",
        "\n",
        "    # 4. Kiểm tra dự đoán với mô hình đã tải\n",
        "\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    from tensorflow.keras.datasets import mnist\n",
        "\n",
        "    (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "    index = 7\n",
        "    image = train_images[index]\n",
        "\n",
        "    print(f\"Kích thước ảnh: {image.shape}\")\n",
        "    print(f\"Nhãn (Label) của ảnh: {train_labels[index]}\")\n",
        "\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(image, cmap=\"gray\")\n",
        "    plt.title(f\"Ảnh (Nhãn: {train_labels[index]})\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Dự đoán: \" + str(np.argmax(predict(image, conv, pool, full))))\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Lỗi: Không tìm thấy file mô hình '{model_filename}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"Lỗi khi tải mô hình: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "13viJofrHn6w",
        "outputId": "09b6d15b-8729-4a53-88b9-6d3b300b8e5d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mô hình đã được tải lại thành công.\n",
            "Kích thước bộ lọc lớp CONV: (5, 2, 2)\n",
            "Kích thước trọng số lớp FC: (10, 980)\n",
            "Kích thước ảnh: (28, 28)\n",
            "Nhãn (Label) của ảnh: 3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dự đoán: 3\n"
          ]
        }
      ]
    }
  ]
}